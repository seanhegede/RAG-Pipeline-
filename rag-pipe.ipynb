{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71fc52c-0334-49de-83f6-55c808461f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 21:35:23,967 - INFO - [Use pytorch device_name: cpu]\n",
      "2025-07-20 21:35:23,968 - INFO - [Load pretrained SentenceTransformer: all-MiniLM-L6-v2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Optimized Fast RAG System...\n",
      "✅ Ollama connection verified\n",
      "📦 Loading embedding model...\n",
      "✅ Embedding model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 21:35:24,387 - WARNING - [Building search indices...]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1615 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 21:35:26,330 - WARNING - [Indices built: 1260 keywords]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FAISS index built (1615 vectors)\n",
      "✅ Enhanced Fast RAG Ready!\n",
      "   📚 1615 chunks loaded\n",
      "   🔍 Hybrid retrieval + lightweight reranking enabled\n",
      "   🔄 5-strategy fallback system active\n",
      "\n",
      "============================================================\n",
      "🤖 ENHANCED FAST RAG SYSTEM READY\n",
      "============================================================\n",
      "✨ New features:\n",
      "  • Lightweight reranking for better relevance\n",
      "  • 5-strategy fallback system\n",
      "  • Enhanced performance monitoring\n",
      "  • Improved caching and conversation context\n",
      "\n",
      "💬 Commands:\n",
      "  • Type questions naturally\n",
      "  • 'stats' - detailed performance info\n",
      "  • 'debug' - show system internals\n",
      "  • 'clear' - clear cache & history\n",
      "  • 'quit' - exit\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "💬 Ask:  What is return on equity?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf7b2c1fe4d4414adc7cef040f179ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 ANSWER\n",
      "--------------------------------------------------\n",
      "Return on equity (ROE) is calculated by dividing a company’s net income by its owners’ equity [Source 1]. It represents the return generated for each dollar of shareholder investment. According to the provided text, a higher level of financial leverage (debt) can lead to a greater return on equity, *even if* net income remains the same [Source 1]. For example, XYZ Software with $100 million in equity and $15 million in net income had a 15% ROE. However, if XYZ had only $50 million in equity (and the same $15 million net income), its ROE would increase to 30% [Source 1]. \n",
      "\n",
      "In essence, ROE measures how efficiently a company is using shareholders’ investments to generate profits [Source 1].\n",
      "\n",
      "📚 SOURCES (8)\n",
      "1. Financial Accounting 101 (relevance: 0.292)\n",
      "2. Financial Accounting 101 (relevance: 0.284)\n",
      "3. Financial Accounting 101 (relevance: 0.281)\n",
      "--------------------------------------------------\n",
      "Time: 437.34s (avg: 437.34s)\n",
      "Cache: 0% | Fallback: 0% | Rerank: 0%\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "💬 Ask:  What is the capital of Moldova?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9998a2fc6d714d65b33409c6e8fa6a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 21:43:44,741 - WARNING - [Triggering fallback: chunks=1, max_sim=0.124]\n",
      "2025-07-20 21:43:44,741 - WARNING - [Executing fallback for query: 'What is the capital of Moldova?...']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "032aac5f210e438f989866c757311c67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 21:43:44,761 - WARNING - [Fallback SUCCESS: Query Expansion found 3 results]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 ANSWER\n",
      "--------------------------------------------------\n",
      "This is a trick question! The provided documents are all about real estate investment and cap rate analysis. They do *not* contain information about the capital of Moldova. \n",
      "\n",
      "The capital of Moldova is **Chisinau** (also spelled Chișinău). \n",
      "\n",
      "The question is designed to test if you can identify irrelevant information and avoid answering based on the provided context.\n",
      "\n",
      "📚 SOURCES (3)\n",
      "1. Selling or Renting 1416 Manchester (relevance: 0.189, via Query Expansion)\n",
      "2. 2517 Bailey Drive Cap Rate Analysis Feb 2025 (relevance: 0.183, via Query Expansion)\n",
      "3. 2745 Saxon Drive Cap Rate Analysis  Feb 2025 (relevance: 0.175, via Query Expansion)\n",
      "--------------------------------------------------\n",
      "Time: 349.39s (avg: 393.36s)\n",
      "Cache: 0% | Fallback: 50% | Rerank: 0%\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "💬 Ask:  What is the internal rate of return and its relationship to return on equity?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7ba2f153b0456189cacc98219d709b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 ANSWER\n",
      "--------------------------------------------------\n",
      "## Return on Equity (ROE) and its Relationship to Internal Rate of Return (IRR)\n",
      "\n",
      "Based on the provided documents, here's a comprehensive explanation of Return on Equity (ROE) and its connection to Internal Rate of Return (IRR):\n",
      "\n",
      "**What is Return on Equity (ROE)?**\n",
      "\n",
      "ROE is a profitability ratio that measures how much profit a company generates with the money shareholders have invested.  [Source 1] defines it as net income divided by owners’ equity.  Essentially, it shows how efficiently a company is using shareholder investments to generate earnings. A higher ROE generally indicates better performance.  [Source 1] highlights that increasing debt (financial leverage) can *increase* ROE, but also increases risk.  The example in [Source 1] demonstrates this:\n",
      "\n",
      "*   **Scenario 1:** $200M assets, $100M liabilities, $100M equity, $15M net income = 15% ROE ($15M / $100M)\n",
      "*   **Scenario 2:** $200M assets, $150M liabilities, $50M equity, $15M net income = 30% ROE ($15M / $50M)\n",
      "\n",
      "**What is Internal Rate of Return (IRR)?**\n",
      "\n",
      "While the provided documents don't *fully* define IRR, [Source 4] mentions it in the context of ROI calculation.  IRR is a discount rate that makes the net present value (NPV) of all cash flows from a particular project equal to zero. In simpler terms, it's the expected annual rate of return that an investment is expected to yield.  It's a more sophisticated measure than simple ROI because it considers the *time value of money* – the idea that money received today is worth more than money received in the future.\n",
      "\n",
      "**How are ROE and IRR Related?**\n",
      "\n",
      "The documents don't explicitly state a direct mathematical relationship between ROE and IRR. However, they are both measures of investment return, and understanding both is crucial for making informed investment decisions. Here's how they connect:\n",
      "\n",
      "*   **Both measure profitability:** Both ROE and IRR aim to quantify the return generated from an investment.\n",
      "*   **IRR is project-specific, ROE is company-wide:** ROE is a broad measure of a company's overall profitability based on shareholder equity. IRR, on the other hand, is calculated for *specific* projects or investments.  You might calculate the IRR of a new rental property, but ROE applies to the entire company.\n",
      "*   **IRR informs investment decisions that *impact* ROE:**  When a company invests in projects with high IRRs, it *should* lead to increased net income, which in turn *increases* ROE.  A company's overall ROE is the result of the collective performance of all its investments, each with its own IRR.\n",
      "*   **Tax benefits impact both:** [Source 4] specifically mentions that tax savings are part of the ROI calculation in property investments. These tax savings would also be factored into the IRR calculation for a property investment, and ultimately contribute to the company's net income and ROE.\n",
      "\n",
      "**In essence:** IRR is a tool used to evaluate individual investment opportunities, while ROE is a summary measure of how effectively a company is using shareholder investments to generate profit.  Successful projects with high IRRs contribute to a higher ROE.\n",
      "\n",
      "\n",
      "\n",
      "**Important Note:** The provided documents offer a limited explanation of IRR. A full understanding of IRR requires knowledge of discounted cash flow analysis and net present value calculations.\n",
      "\n",
      "📚 SOURCES (8)\n",
      "1. Financial Accounting 101 (relevance: 0.279)\n",
      "2. 2517 Bailey Drive Cap Rate Analysis Feb 2025 (relevance: 0.269)\n",
      "3. 2745 Saxon Drive Cap Rate Analysis  Feb 2025 (relevance: 0.260)\n",
      "--------------------------------------------------\n",
      "Time: 639.04s (avg: 475.26s)\n",
      "Cache: 0% | Fallback: 33% | Rerank: 0%\n"
     ]
    }
   ],
   "source": [
    "#Loading Jupyter Notebooks: jupyter notebook --ip=0.0.0.0 --port=8889 --no-browser --allow-root\n",
    "#SSH EC2 Instance channel: ssh -i /home/seanhegede/myenv/noko.pem -L 10006:localhost:8889 -L 11434:localhost:11434 ubuntu@18.xxx.xx.xx\n",
    "#Jupyter \n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "import logging\n",
    "from collections import defaultdict, deque\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import hashlib\n",
    "from functools import lru_cache\n",
    "import threading\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "EMBEDDINGS_FILE = \"/home/ubuntu/scraper_deep/deep_embeddings.json\"\n",
    "GEMMA_URL = \"http://localhost:11434/api/generate\"\n",
    "GEMMA_MODEL = \"gemma3:27b\"\n",
    "\n",
    "# Performance settings\n",
    "TOP_K = 8\n",
    "MIN_SIMILARITY = 0.12\n",
    "MAX_CONTEXT_LENGTH = 12000\n",
    "HYBRID_WEIGHT_SEMANTIC = 0.6\n",
    "HYBRID_WEIGHT_KEYWORD = 0.25\n",
    "HYBRID_WEIGHT_RERANK = 0.15\n",
    "CONVERSATION_HISTORY = 5\n",
    "RERANK_TOP_K = 16  # Get more candidates for reranking\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LightweightReranker:\n",
    "    \"\"\"Fast reranking based on multiple lightweight signals.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stopwords = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}\n",
    "        \n",
    "    def rerank_chunks(self, query: str, chunks: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Lightweight reranking using multiple fast signals.\"\"\"\n",
    "        if len(chunks) <= 2:\n",
    "            return chunks\n",
    "            \n",
    "        query_lower = query.lower()\n",
    "        query_words = set(re.findall(r'\\b\\w+\\b', query_lower))\n",
    "        query_words = query_words - self.stopwords\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            text = chunk.get('text', '').lower()\n",
    "            title = chunk.get('title', '').lower()\n",
    "            \n",
    "            # Signal 1: Exact phrase matches (highest weight)\n",
    "            exact_matches = self._count_exact_phrases(query_lower, text)\n",
    "            \n",
    "            # Signal 2: Title relevance\n",
    "            title_score = self._calculate_title_relevance(query_words, title)\n",
    "            \n",
    "            # Signal 3: Word density\n",
    "            density_score = self._calculate_word_density(query_words, text)\n",
    "            \n",
    "            # Signal 4: Position of matches (earlier = better)\n",
    "            position_score = self._calculate_position_score(query_words, text)\n",
    "            \n",
    "            # Signal 5: Text quality (length, structure)\n",
    "            quality_score = self._calculate_text_quality(text)\n",
    "            \n",
    "            # Combine signals\n",
    "            rerank_score = (\n",
    "                exact_matches * 0.4 +\n",
    "                title_score * 0.25 +\n",
    "                density_score * 0.15 +\n",
    "                position_score * 0.1 +\n",
    "                quality_score * 0.1\n",
    "            )\n",
    "            \n",
    "            # Blend with original similarity\n",
    "            original_sim = chunk.get('similarity', 0)\n",
    "            chunk['final_score'] = (original_sim * 0.7) + (rerank_score * 0.3)\n",
    "            chunk['rerank_signals'] = {\n",
    "                'exact': exact_matches,\n",
    "                'title': title_score,\n",
    "                'density': density_score,\n",
    "                'position': position_score,\n",
    "                'quality': quality_score\n",
    "            }\n",
    "        \n",
    "        # Sort by final score\n",
    "        chunks.sort(key=lambda x: x.get('final_score', 0), reverse=True)\n",
    "        return chunks\n",
    "    \n",
    "    def _count_exact_phrases(self, query: str, text: str) -> float:\n",
    "        \"\"\"Count exact phrase matches.\"\"\"\n",
    "        if len(query) < 10:\n",
    "            return 1.0 if query in text else 0.0\n",
    "        \n",
    "        # Split query into meaningful phrases\n",
    "        words = query.split()\n",
    "        phrases = []\n",
    "        \n",
    "        # 2-3 word phrases\n",
    "        for i in range(len(words) - 1):\n",
    "            if len(words[i]) > 2 and len(words[i+1]) > 2:\n",
    "                phrases.append(f\"{words[i]} {words[i+1]}\")\n",
    "        \n",
    "        for i in range(len(words) - 2):\n",
    "            if all(len(w) > 2 for w in words[i:i+3]):\n",
    "                phrases.append(f\"{words[i]} {words[i+1]} {words[i+2]}\")\n",
    "        \n",
    "        matches = sum(1 for phrase in phrases if phrase in text)\n",
    "        return matches / max(len(phrases), 1)\n",
    "    \n",
    "    def _calculate_title_relevance(self, query_words: set, title: str) -> float:\n",
    "        \"\"\"Calculate title relevance.\"\"\"\n",
    "        if not title or not query_words:\n",
    "            return 0.0\n",
    "        \n",
    "        title_words = set(re.findall(r'\\b\\w+\\b', title.lower()))\n",
    "        title_words = title_words - self.stopwords\n",
    "        \n",
    "        if not title_words:\n",
    "            return 0.0\n",
    "        \n",
    "        overlap = len(query_words.intersection(title_words))\n",
    "        return overlap / len(query_words)\n",
    "    \n",
    "    def _calculate_word_density(self, query_words: set, text: str) -> float:\n",
    "        \"\"\"Calculate density of query words in text.\"\"\"\n",
    "        if not query_words or not text:\n",
    "            return 0.0\n",
    "        \n",
    "        text_words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        if len(text_words) < 10:\n",
    "            return 0.0\n",
    "        \n",
    "        matches = sum(1 for word in text_words if word in query_words)\n",
    "        return min(matches / len(text_words) * 100, 1.0)  # Cap at 1.0\n",
    "    \n",
    "    def _calculate_position_score(self, query_words: set, text: str) -> float:\n",
    "        \"\"\"Earlier matches score higher.\"\"\"\n",
    "        if not query_words or not text:\n",
    "            return 0.0\n",
    "        \n",
    "        words = text.lower().split()\n",
    "        first_match_pos = len(words)\n",
    "        \n",
    "        for i, word in enumerate(words[:100]):  # Only check first 100 words\n",
    "            if word in query_words:\n",
    "                first_match_pos = i\n",
    "                break\n",
    "        \n",
    "        if first_match_pos == len(words):\n",
    "            return 0.0\n",
    "        \n",
    "        return max(0, (50 - first_match_pos) / 50)  # Normalize to 0-1\n",
    "    \n",
    "    def _calculate_text_quality(self, text: str) -> float:\n",
    "        \"\"\"Simple text quality score.\"\"\"\n",
    "        if not text:\n",
    "            return 0.0\n",
    "        \n",
    "        words = text.split()\n",
    "        sentences = text.split('.')\n",
    "        \n",
    "        # Prefer medium-length texts\n",
    "        length_score = 1.0 if 50 <= len(words) <= 300 else 0.5\n",
    "        \n",
    "        # Prefer well-structured text\n",
    "        structure_score = 1.0 if len(sentences) > 2 else 0.5\n",
    "        \n",
    "        return (length_score + structure_score) / 2\n",
    "\n",
    "class ImprovedFallbackSystem:\n",
    "    \"\"\"Enhanced fallback with better strategies and debugging.\"\"\"\n",
    "    \n",
    "    def __init__(self, rag_system):\n",
    "        self.rag_system = rag_system\n",
    "        self.query_expansions = {\n",
    "            # Technology terms\n",
    "            'ai': ['artificial intelligence', 'machine learning', 'neural network', 'algorithm'],\n",
    "            'ml': ['machine learning', 'artificial intelligence', 'data science', 'algorithm'],\n",
    "            'api': ['interface', 'endpoint', 'service', 'integration', 'rest'],\n",
    "            'web': ['website', 'internet', 'online', 'browser', 'http'],\n",
    "            'data': ['dataset', 'information', 'database', 'analytics', 'statistics'],\n",
    "            'python': ['programming', 'code', 'script', 'development'],\n",
    "            'javascript': ['js', 'programming', 'web development', 'frontend'],\n",
    "            'database': ['db', 'sql', 'storage', 'data', 'table'],\n",
    "            \n",
    "            # Business terms\n",
    "            'business': ['company', 'enterprise', 'organization', 'commercial'],\n",
    "            'marketing': ['advertising', 'promotion', 'sales', 'branding'],\n",
    "            'finance': ['money', 'budget', 'investment', 'financial', 'cost'],\n",
    "            'strategy': ['plan', 'approach', 'method', 'tactics'],\n",
    "            \n",
    "            # General terms\n",
    "            'how': ['method', 'way', 'process', 'steps', 'guide'],\n",
    "            'why': ['reason', 'purpose', 'cause', 'explanation'],\n",
    "            'what': ['definition', 'meaning', 'explanation', 'description'],\n",
    "            'best': ['optimal', 'recommended', 'top', 'effective'],\n",
    "        }\n",
    "    \n",
    "    def execute_fallback(self, query: str, original_chunks: List[Dict] = None) -> List[Dict]:\n",
    "        \"\"\"Execute enhanced fallback strategies with detailed logging.\"\"\"\n",
    "        logger.warning(f\"Executing fallback for query: '{query[:50]}...'\")\n",
    "        \n",
    "        strategies = [\n",
    "            ('Query Expansion', self._query_expansion),\n",
    "            ('Relaxed Similarity', self._relaxed_search),\n",
    "            ('Individual Keywords', self._keyword_search),\n",
    "            ('Fuzzy Matching', self._fuzzy_search),\n",
    "            ('Partial Content', self._partial_content_search),\n",
    "        ]\n",
    "        \n",
    "        for strategy_name, strategy_func in strategies:\n",
    "            try:\n",
    "                results = strategy_func(query)\n",
    "                if results and len(results) >= 2:  # Require at least 2 results\n",
    "                    logger.warning(f\"Fallback SUCCESS: {strategy_name} found {len(results)} results\")\n",
    "                    # Add fallback metadata\n",
    "                    for result in results:\n",
    "                        result['fallback_method'] = strategy_name\n",
    "                    return results\n",
    "                elif results:\n",
    "                    logger.warning(f\"Fallback PARTIAL: {strategy_name} found {len(results)} results (need 2+)\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Fallback ERROR in {strategy_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        logger.warning(\"All fallback strategies failed\")\n",
    "        return []\n",
    "    \n",
    "    def _query_expansion(self, query: str) -> List[Dict]:\n",
    "        \"\"\"Expand query with related terms.\"\"\"\n",
    "        query_lower = query.lower()\n",
    "        expanded_terms = []\n",
    "        \n",
    "        # Add expansions for found terms\n",
    "        for term, expansions in self.query_expansions.items():\n",
    "            if term in query_lower:\n",
    "                expanded_terms.extend(expansions[:2])  # Limit expansions\n",
    "        \n",
    "        if expanded_terms:\n",
    "            # Try different expansion strategies\n",
    "            strategies = [\n",
    "                f\"{query} {' '.join(expanded_terms[:3])}\",  # Add terms\n",
    "                ' '.join(expanded_terms[:4]),  # Just expanded terms\n",
    "                f\"{' '.join(query.split()[:3])} {' '.join(expanded_terms[:2])}\"  # Mix\n",
    "            ]\n",
    "            \n",
    "            for expanded_query in strategies:\n",
    "                results = self.rag_system.retrieve_context(expanded_query)\n",
    "                if results and len(results) >= 2:\n",
    "                    return results[:TOP_K]\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    def _relaxed_search(self, query: str) -> List[Dict]:\n",
    "        \"\"\"Search with progressively relaxed thresholds.\"\"\"\n",
    "        global MIN_SIMILARITY\n",
    "        original_min = MIN_SIMILARITY\n",
    "        \n",
    "        thresholds = [0.08, 0.05, 0.03]\n",
    "        \n",
    "        try:\n",
    "            for threshold in thresholds:\n",
    "                MIN_SIMILARITY = threshold\n",
    "                results = self.rag_system.retrieve_context(query)\n",
    "                if results and len(results) >= 2:\n",
    "                    return results[:TOP_K]\n",
    "        finally:\n",
    "            MIN_SIMILARITY = original_min\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    def _keyword_search(self, query: str) -> List[Dict]:\n",
    "        \"\"\"Search using individual important keywords.\"\"\"\n",
    "        # Extract meaningful keywords\n",
    "        words = re.findall(r'\\b[a-zA-Z]{3,}\\b', query.lower())\n",
    "        stopwords = {'the', 'and', 'are', 'you', 'what', 'how', 'when', 'where', 'why', 'this', 'that', 'with', 'for', 'can', 'could', 'would', 'should', 'will', 'may', 'might', 'must', 'have', 'has', 'had', 'was', 'were', 'been', 'said', 'from', 'they', 'them', 'than', 'only', 'even', 'also', 'back', 'other', 'many', 'then', 'well', 'some', 'like', 'just', 'very', 'more'}\n",
    "        \n",
    "        important_words = [w for w in words if w not in stopwords and len(w) > 3]\n",
    "        \n",
    "        if len(important_words) >= 1:\n",
    "            # Try different keyword combinations\n",
    "            strategies = [\n",
    "                ' '.join(important_words[:4]),  # Top 4 keywords\n",
    "                ' '.join(important_words[:2]),  # Top 2 keywords  \n",
    "                important_words[0] if important_words else query  # Single most important\n",
    "            ]\n",
    "            \n",
    "            for keyword_query in strategies:\n",
    "                if keyword_query.strip():\n",
    "                    results = self.rag_system.retrieve_context(keyword_query)\n",
    "                    if results:\n",
    "                        return results[:TOP_K]\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    def _fuzzy_search(self, query: str) -> List[Dict]:\n",
    "        \"\"\"Fuzzy matching against chunk titles and content.\"\"\"\n",
    "        query_lower = query.lower()\n",
    "        fuzzy_matches = []\n",
    "        \n",
    "        for i, chunk in enumerate(self.rag_system.knowledge_base[:1000]):  # Limit search\n",
    "            title = chunk.get('title', '').lower()\n",
    "            text_preview = chunk.get('text', '')[:200].lower()\n",
    "            \n",
    "            # Check fuzzy similarity with title\n",
    "            title_sim = SequenceMatcher(None, query_lower, title).ratio()\n",
    "            text_sim = SequenceMatcher(None, query_lower, text_preview).ratio()\n",
    "            \n",
    "            max_sim = max(title_sim, text_sim)\n",
    "            if max_sim > 0.3:  # Fuzzy threshold\n",
    "                chunk_copy = chunk.copy()\n",
    "                chunk_copy['similarity'] = max_sim\n",
    "                fuzzy_matches.append(chunk_copy)\n",
    "        \n",
    "        # Sort by fuzzy similarity\n",
    "        fuzzy_matches.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "        return fuzzy_matches[:TOP_K]\n",
    "    \n",
    "    def _partial_content_search(self, query: str) -> List[Dict]:\n",
    "        \"\"\"Search for partial content matches.\"\"\"\n",
    "        # Extract noun phrases and important terms\n",
    "        query_parts = []\n",
    "        \n",
    "        # Split on common question words and conjunctions\n",
    "        parts = re.split(r'\\b(?:what|how|when|where|why|and|or|but|if|then)\\b', query.lower())\n",
    "        for part in parts:\n",
    "            cleaned = part.strip()\n",
    "            if len(cleaned) > 5:\n",
    "                query_parts.append(cleaned)\n",
    "        \n",
    "        # Try each part as a separate query\n",
    "        for part in query_parts[:3]:  # Limit attempts\n",
    "            if part.strip():\n",
    "                results = self.rag_system.retrieve_context(part.strip())\n",
    "                if results:\n",
    "                    return results[:TOP_K]\n",
    "        \n",
    "        return []\n",
    "\n",
    "class FastRAG:\n",
    "    \"\"\"Enhanced RAG system with lightweight reranking and improved fallbacks.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.index = None\n",
    "        self.knowledge_base = []\n",
    "        self.hybrid_retriever = None\n",
    "        self.conversation = ConversationManager()\n",
    "        self.cache = FastCache()\n",
    "        self.fallback_system = None\n",
    "        self.reranker = LightweightReranker()\n",
    "        self.ready = False\n",
    "        \n",
    "        # Enhanced stats\n",
    "        self.stats = {\n",
    "            'queries': 0, \n",
    "            'cache_hits': 0, \n",
    "            'fallbacks': 0,\n",
    "            'rerank_improvements': 0,\n",
    "            'avg_response_time': 0,\n",
    "            'total_response_time': 0\n",
    "        }\n",
    "    \n",
    "    def setup(self) -> bool:\n",
    "        \"\"\"Quick setup with better error handling.\"\"\"\n",
    "        print(\"🚀 Starting Optimized Fast RAG System...\")\n",
    "        \n",
    "        # Check Ollama\n",
    "        try:\n",
    "            response = requests.get(\"http://localhost:11434/api/tags\", timeout=3)\n",
    "            models = response.json().get(\"models\", [])\n",
    "            if not any(GEMMA_MODEL in model['name'] for model in models):\n",
    "                print(f\"❌ Model {GEMMA_MODEL} not found\")\n",
    "                return False\n",
    "            print(\"✅ Ollama connection verified\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Ollama not available: {e}\")\n",
    "            return False\n",
    "        \n",
    "        # Load model with better error handling\n",
    "        try:\n",
    "            print(\"📦 Loading embedding model...\")\n",
    "            self.model = SentenceTransformer(MODEL_NAME)\n",
    "            print(\"✅ Embedding model loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Model loading failed: {e}\")\n",
    "            return False\n",
    "        \n",
    "        # Load knowledge base\n",
    "        if not self._load_knowledge_base():\n",
    "            return False\n",
    "        \n",
    "        # Build index\n",
    "        if not self._build_index():\n",
    "            return False\n",
    "        \n",
    "        self.fallback_system = ImprovedFallbackSystem(self)\n",
    "        self.ready = True\n",
    "        \n",
    "        print(f\"✅ Enhanced Fast RAG Ready!\")\n",
    "        print(f\"   📚 {len(self.knowledge_base)} chunks loaded\")\n",
    "        print(f\"   🔍 Hybrid retrieval + lightweight reranking enabled\")\n",
    "        print(f\"   🔄 5-strategy fallback system active\")\n",
    "        return True\n",
    "    \n",
    "    def retrieve_context(self, query: str) -> List[Dict]:\n",
    "        \"\"\"Enhanced context retrieval with reranking.\"\"\"\n",
    "        try:\n",
    "            # Get more candidates for reranking\n",
    "            query_embedding = self.model.encode([query]).astype('float32')\n",
    "            faiss.normalize_L2(query_embedding)\n",
    "            \n",
    "            scores, indices = self.index.search(query_embedding, min(RERANK_TOP_K, len(self.knowledge_base)))\n",
    "            \n",
    "            semantic_results = []\n",
    "            for score, idx in zip(scores[0], indices[0]):\n",
    "                if idx != -1 and score > 0.08:  # Lower threshold for reranking\n",
    "                    semantic_results.append((idx, float(score)))\n",
    "            \n",
    "            # Get initial results\n",
    "            if self.hybrid_retriever and semantic_results:\n",
    "                initial_chunks = self.hybrid_retriever.fast_search(query, semantic_results)\n",
    "            else:\n",
    "                initial_chunks = []\n",
    "                for idx, score in semantic_results[:TOP_K]:\n",
    "                    chunk = self.knowledge_base[idx].copy()\n",
    "                    chunk['similarity'] = score\n",
    "                    initial_chunks.append(chunk)\n",
    "            \n",
    "            # Apply lightweight reranking\n",
    "            if len(initial_chunks) > 2:\n",
    "                reranked_chunks = self.reranker.rerank_chunks(query, initial_chunks)\n",
    "                \n",
    "                # Check if reranking improved results\n",
    "                if reranked_chunks != initial_chunks:\n",
    "                    self.stats['rerank_improvements'] += 1\n",
    "                \n",
    "                return reranked_chunks[:TOP_K]\n",
    "            \n",
    "            return initial_chunks[:TOP_K]\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Context retrieval failed: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def ask(self, query: str):\n",
    "        \"\"\"Process query with enhanced fallback and reranking.\"\"\"\n",
    "        if not self.ready:\n",
    "            print(\"❌ System not ready\")\n",
    "            return\n",
    "        \n",
    "        self.stats['queries'] += 1\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Check cache first\n",
    "        cache_key = FastCache.hash_query(query)\n",
    "        cached_response = self.cache.get(cache_key)\n",
    "        if cached_response:\n",
    "            self.stats['cache_hits'] += 1\n",
    "            print(f\"\\n⚡ CACHED ANSWER\\n{'-'*50}\")\n",
    "            print(cached_response)\n",
    "            print(f\"{'-'*50}\\nResponse time: <0.01s | From cache\")\n",
    "            return\n",
    "        \n",
    "        # Get conversation context\n",
    "        conv_context = self.conversation.get_context_for_query(query)\n",
    "        \n",
    "        # Retrieve context\n",
    "        chunks = self.retrieve_context(query)\n",
    "        \n",
    "        # Enhanced fallback logic\n",
    "        needs_fallback = (\n",
    "            not chunks or \n",
    "            len(chunks) < 2 or  # Need at least 2 good chunks\n",
    "            (chunks and max(c.get('similarity', 0) for c in chunks) < 0.25)\n",
    "        )\n",
    "        \n",
    "        if needs_fallback:\n",
    "            logger.warning(f\"Triggering fallback: chunks={len(chunks)}, max_sim={max([c.get('similarity', 0) for c in chunks], default=0):.3f}\")\n",
    "            fallback_chunks = self.fallback_system.execute_fallback(query, chunks)\n",
    "            if fallback_chunks and len(fallback_chunks) >= len(chunks):\n",
    "                chunks = fallback_chunks\n",
    "                self.stats['fallbacks'] += 1\n",
    "        \n",
    "        # Build prompt and get answer\n",
    "        prompt = self._build_prompt(query, chunks, conv_context)\n",
    "        answer = self._query_llm(prompt)\n",
    "        \n",
    "        # Cache good responses\n",
    "        if len(answer) > 50 and \"error\" not in answer.lower():\n",
    "            self.cache.set(cache_key, answer)\n",
    "        \n",
    "        # Add to conversation history\n",
    "        self.conversation.add_exchange(query, answer, chunks)\n",
    "        \n",
    "        # Calculate and update stats\n",
    "        response_time = time.time() - start_time\n",
    "        self.stats['total_response_time'] += response_time\n",
    "        self.stats['avg_response_time'] = self.stats['total_response_time'] / self.stats['queries']\n",
    "        \n",
    "        # Display result with enhanced info\n",
    "        print(f\"\\n📝 ANSWER\\n{'-'*50}\")\n",
    "        print(answer)\n",
    "        \n",
    "        if chunks:\n",
    "            print(f\"\\n📚 SOURCES ({len(chunks)})\")\n",
    "            for i, chunk in enumerate(chunks[:3], 1):\n",
    "                similarity = chunk.get('final_score', chunk.get('similarity', 0))\n",
    "                title = chunk.get('title', 'Unknown')\n",
    "                fallback_method = chunk.get('fallback_method', '')\n",
    "                \n",
    "                info_parts = [f\"relevance: {similarity:.3f}\"]\n",
    "                if fallback_method:\n",
    "                    info_parts.append(f\"via {fallback_method}\")\n",
    "                \n",
    "                print(f\"{i}. {title} ({', '.join(info_parts)})\")\n",
    "        \n",
    "        # Enhanced stats display\n",
    "        cache_rate = (self.stats['cache_hits'] / self.stats['queries']) * 100\n",
    "        fallback_rate = (self.stats['fallbacks'] / self.stats['queries']) * 100\n",
    "        rerank_rate = (self.stats['rerank_improvements'] / self.stats['queries']) * 100\n",
    "        \n",
    "        print(f\"{'-'*50}\")\n",
    "        print(f\"Time: {response_time:.2f}s (avg: {self.stats['avg_response_time']:.2f}s)\")\n",
    "        print(f\"Cache: {cache_rate:.0f}% | Fallback: {fallback_rate:.0f}% | Rerank: {rerank_rate:.0f}%\")\n",
    "\n",
    "    # ... [Keep the other methods from the original FastRAG class unchanged]\n",
    "    def _load_knowledge_base(self) -> bool:\n",
    "        \"\"\"Load knowledge base quickly.\"\"\"\n",
    "        if not os.path.exists(EMBEDDINGS_FILE):\n",
    "            print(f\"❌ File not found: {EMBEDDINGS_FILE}\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            with open(EMBEDDINGS_FILE, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            self.knowledge_base = []\n",
    "            if isinstance(data, dict):\n",
    "                for doc_id, item in data.items():\n",
    "                    if item.get('text') and item.get('embedding') and len(item.get('text', '').strip()) > 30:\n",
    "                        self.knowledge_base.append({\n",
    "                            'id': doc_id,\n",
    "                            'text': item['text'],\n",
    "                            'embedding': np.array(item['embedding']),\n",
    "                            'title': item.get('page_title', 'Unknown'),\n",
    "                            'url': item.get('url', '')\n",
    "                        })\n",
    "            else:\n",
    "                for i, item in enumerate(data):\n",
    "                    if isinstance(item, dict) and item.get('text') and item.get('embedding') and len(item.get('text', '').strip()) > 30:\n",
    "                        self.knowledge_base.append({\n",
    "                            'id': item.get('id', f'doc_{i}'),\n",
    "                            'text': item['text'],\n",
    "                            'embedding': np.array(item['embedding']),\n",
    "                            'title': item.get('page_title', 'Unknown'),\n",
    "                            'url': item.get('url', '')\n",
    "                        })\n",
    "            \n",
    "            print(f\"✅ Loaded {len(self.knowledge_base)} chunks\")\n",
    "            return len(self.knowledge_base) > 0\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load knowledge base: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _build_index(self) -> bool:\n",
    "        \"\"\"Build FAISS index quickly.\"\"\"\n",
    "        if not self.knowledge_base:\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            embeddings = np.vstack([chunk['embedding'] for chunk in self.knowledge_base]).astype('float32')\n",
    "            faiss.normalize_L2(embeddings)\n",
    "            \n",
    "            # Use simple flat index for speed\n",
    "            self.index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "            self.index.add(embeddings)\n",
    "            \n",
    "            # Build hybrid retriever\n",
    "            self.hybrid_retriever = FastHybridRetriever(self.knowledge_base)\n",
    "            \n",
    "            print(f\"✅ FAISS index built ({embeddings.shape[0]} vectors)\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Index building failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _build_prompt(self, query: str, chunks: List[Dict], context: str = \"\") -> str:\n",
    "        \"\"\"Build efficient prompt.\"\"\"\n",
    "        if not chunks:\n",
    "            return f\"{context}I don't have specific information to answer: '{query}'. Could you rephrase or ask about a related topic?\"\n",
    "        \n",
    "        # Build context efficiently\n",
    "        context_parts = []\n",
    "        total_length = 0\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            if total_length >= MAX_CONTEXT_LENGTH:\n",
    "                break\n",
    "            \n",
    "            text = chunk['text'].strip()\n",
    "            source = chunk.get('title', 'Unknown')\n",
    "            \n",
    "            chunk_text = f\"[Source {i+1}: {source}]\\n{text}\\n\\n\"\n",
    "            \n",
    "            if total_length + len(chunk_text) <= MAX_CONTEXT_LENGTH:\n",
    "                context_parts.append(chunk_text)\n",
    "                total_length += len(chunk_text)\n",
    "            else:\n",
    "                # Add partial content if there's room\n",
    "                remaining = MAX_CONTEXT_LENGTH - total_length\n",
    "                if remaining > 200:\n",
    "                    partial = text[:remaining-50] + \"...\"\n",
    "                    context_parts.append(f\"[Source {i+1}: {source}]\\n{partial}\\n\\n\")\n",
    "                break\n",
    "        \n",
    "        full_context = \"\".join(context_parts)\n",
    "        \n",
    "        return f\"\"\"{context}Based on the provided context, answer the question comprehensively. Cite sources using [Source X] format.\n",
    "\n",
    "CONTEXT:\n",
    "{full_context}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "    \n",
    "    def _query_llm(self, prompt: str) -> str:\n",
    "        \"\"\"Fast LLM query without retries.\"\"\"\n",
    "        try:\n",
    "            payload = {\n",
    "                \"model\": GEMMA_MODEL,\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False,\n",
    "                \"options\": {\n",
    "                    \"temperature\": 0.2,\n",
    "                    \"top_p\": 0.9,\n",
    "                    \"max_tokens\": 1000\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            response = requests.post(GEMMA_URL, json=payload, timeout=None)\n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                answer = result.get('response', '').strip()\n",
    "                return answer if answer else \"Unable to generate response.\"\n",
    "            else:\n",
    "                return \"Error communicating with language model.\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"LLM query failed: {e}\")\n",
    "            return \"Technical error occurred.\"\n",
    "\n",
    "# Keep the other classes unchanged (ConversationManager, FastCache, FastTextProcessor, FastHybridRetriever)\n",
    "class ConversationManager:\n",
    "    \"\"\"Manages conversation context and history.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_history: int = CONVERSATION_HISTORY):\n",
    "        self.history = deque(maxlen=max_history)\n",
    "        self.context_keywords = set()\n",
    "        self.current_topic = None\n",
    "        \n",
    "    def add_exchange(self, query: str, answer: str, chunks: List[Dict]):\n",
    "        \"\"\"Add query-answer exchange to history.\"\"\"\n",
    "        exchange = {\n",
    "            'query': query,\n",
    "            'answer': answer,\n",
    "            'keywords': self._extract_keywords(query),\n",
    "            'topics': [chunk.get('title', '') for chunk in chunks[:3]],\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "        self.history.append(exchange)\n",
    "        self._update_context(exchange)\n",
    "    \n",
    "    def _extract_keywords(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract keywords from text.\"\"\"\n",
    "        words = re.findall(r'\\b[a-zA-Z]{3,}\\b', text.lower())\n",
    "        stopwords = {'the', 'and', 'are', 'you', 'what', 'how', 'when', 'where', 'why', 'this', 'that', 'with', 'for', 'can', 'could', 'would', 'should'}\n",
    "        return [w for w in words if w not in stopwords]\n",
    "    \n",
    "    def _update_context(self, exchange: Dict):\n",
    "        \"\"\"Update conversation context.\"\"\"\n",
    "        self.context_keywords.update(exchange['keywords'][:5])\n",
    "        if len(self.context_keywords) > 20:\n",
    "            # Keep most recent keywords\n",
    "            recent_keywords = set()\n",
    "            for ex in list(self.history)[-3:]:\n",
    "                recent_keywords.update(ex['keywords'][:3])\n",
    "            self.context_keywords = recent_keywords\n",
    "    \n",
    "    def get_context_for_query(self, query: str) -> str:\n",
    "        \"\"\"Get relevant context for current query.\"\"\"\n",
    "        if not self.history:\n",
    "            return \"\"\n",
    "        \n",
    "        query_keywords = set(self._extract_keywords(query))\n",
    "        context_parts = []\n",
    "        \n",
    "        # Find related previous exchanges\n",
    "        for exchange in reversed(list(self.history)):\n",
    "            overlap = len(query_keywords.intersection(set(exchange['keywords'])))\n",
    "            if overlap > 0:\n",
    "                context_parts.append(f\"Previously asked: {exchange['query']}\")\n",
    "                if len(context_parts) >= 2:\n",
    "                    break\n",
    "        \n",
    "        if context_parts:\n",
    "            return \"Conversation context: \" + \" | \".join(context_parts) + \"\\n\\n\"\n",
    "        return \"\"\n",
    "\n",
    "class FastCache:\n",
    "    \"\"\"Lightweight caching system.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size: int = 100):\n",
    "        self.cache = {}\n",
    "        self.access_order = deque(maxlen=max_size)\n",
    "        self.max_size = max_size\n",
    "    \n",
    "    def get(self, key: str) -> Optional[str]:\n",
    "        if key in self.cache:\n",
    "            # Move to end (most recently used)\n",
    "            self.access_order.remove(key)\n",
    "            self.access_order.append(key)\n",
    "            return self.cache[key]\n",
    "        return None\n",
    "    \n",
    "    def set(self, key: str, value: str):\n",
    "        if len(self.cache) >= self.max_size and key not in self.cache:\n",
    "            # Remove least recently used\n",
    "            old_key = self.access_order.popleft()\n",
    "            del self.cache[old_key]\n",
    "        \n",
    "        self.cache[key] = value\n",
    "        if key in self.access_order:\n",
    "            self.access_order.remove(key)\n",
    "        self.access_order.append(key)\n",
    "    \n",
    "    @staticmethod\n",
    "    def hash_query(query: str) -> str:\n",
    "        return hashlib.md5(query.lower().strip().encode()).hexdigest()[:12]\n",
    "\n",
    "class FastTextProcessor:\n",
    "    \"\"\"Streamlined text processing.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stopwords = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those', 'a', 'an'}\n",
    "    \n",
    "    @lru_cache(maxsize=500)\n",
    "    def extract_keywords(self, text: str) -> Tuple[str, ...]:\n",
    "        \"\"\"Extract and cache keywords.\"\"\"\n",
    "        words = re.findall(r'\\b[a-zA-Z0-9]{2,}\\b', text.lower())\n",
    "        keywords = tuple(w for w in words if w not in self.stopwords and len(w) > 2)\n",
    "        return keywords[:15]  # Limit keywords\n",
    "    \n",
    "    @lru_cache(maxsize=300)\n",
    "    def extract_phrases(self, text: str) -> Tuple[str, ...]:\n",
    "        \"\"\"Extract meaningful phrases.\"\"\"\n",
    "        # Simple phrase extraction - consecutive words\n",
    "        words = text.lower().split()\n",
    "        phrases = []\n",
    "        for i in range(len(words) - 1):\n",
    "            if len(words[i]) > 2 and len(words[i+1]) > 2:\n",
    "                phrase = f\"{words[i]} {words[i+1]}\"\n",
    "                if words[i] not in self.stopwords and words[i+1] not in self.stopwords:\n",
    "                    phrases.append(phrase)\n",
    "        return tuple(phrases[:10])\n",
    "\n",
    "class FastHybridRetriever:\n",
    "    \"\"\"Fast hybrid retrieval system.\"\"\"\n",
    "    \n",
    "    def __init__(self, knowledge_base: List[Dict]):\n",
    "        self.knowledge_base = knowledge_base\n",
    "        self.text_processor = FastTextProcessor()\n",
    "        self.keyword_index = defaultdict(set)\n",
    "        self.phrase_index = defaultdict(set)\n",
    "        self.tfidf_vectorizer = None\n",
    "        self.tfidf_matrix = None\n",
    "        self._build_indices()\n",
    "    \n",
    "    def _build_indices(self):\n",
    "        \"\"\"Build search indices quickly.\"\"\"\n",
    "        logger.warning(\"Building search indices...\")\n",
    "        \n",
    "        documents = []\n",
    "        for idx, chunk in enumerate(self.knowledge_base):\n",
    "            text = chunk.get('text', '')\n",
    "            if not text:\n",
    "                documents.append(\"\")\n",
    "                continue\n",
    "                \n",
    "            # Build keyword index\n",
    "            keywords = self.text_processor.extract_keywords(text)\n",
    "            for keyword in keywords:\n",
    "                self.keyword_index[keyword].add(idx)\n",
    "            \n",
    "            # Build phrase index\n",
    "            phrases = self.text_processor.extract_phrases(text)\n",
    "            for phrase in phrases:\n",
    "                self.phrase_index[phrase].add(idx)\n",
    "            \n",
    "            documents.append(text.lower())\n",
    "        \n",
    "        # Build TF-IDF (simplified)\n",
    "        if documents:\n",
    "            self.tfidf_vectorizer = TfidfVectorizer(\n",
    "                max_features=10000,\n",
    "                ngram_range=(1, 2),\n",
    "                stop_words='english',\n",
    "                min_df=2,\n",
    "                max_df=0.8\n",
    "            )\n",
    "            try:\n",
    "                self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(documents)\n",
    "            except:\n",
    "                self.tfidf_matrix = None\n",
    "        \n",
    "        logger.warning(f\"Indices built: {len(self.keyword_index)} keywords\")\n",
    "    \n",
    "    def fast_search(self, query: str, semantic_results: List[Tuple[int, float]]) -> List[Dict]:\n",
    "        \"\"\"Fast hybrid search combining all methods.\"\"\"\n",
    "        # Keyword search\n",
    "        query_keywords = self.text_processor.extract_keywords(query)\n",
    "        keyword_scores = defaultdict(float)\n",
    "        \n",
    "        for keyword in query_keywords:\n",
    "            for doc_idx in self.keyword_index.get(keyword, set()):\n",
    "                keyword_scores[doc_idx] += 1.0 / len(query_keywords)\n",
    "        \n",
    "        # Phrase search\n",
    "        query_phrases = self.text_processor.extract_phrases(query)\n",
    "        for phrase in query_phrases:\n",
    "            for doc_idx in self.phrase_index.get(phrase, set()):\n",
    "                keyword_scores[doc_idx] += 2.0 / max(len(query_phrases), 1)\n",
    "        \n",
    "        # TF-IDF search\n",
    "        tfidf_scores = {}\n",
    "        if self.tfidf_vectorizer and self.tfidf_matrix is not None:\n",
    "            try:\n",
    "                query_vec = self.tfidf_vectorizer.transform([query.lower()])\n",
    "                similarities = cosine_similarity(query_vec, self.tfidf_matrix).flatten()\n",
    "                for idx, sim in enumerate(similarities):\n",
    "                    if sim > 0.05:\n",
    "                        tfidf_scores[idx] = sim\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Combine scores\n",
    "        final_scores = defaultdict(float)\n",
    "        all_indices = set()\n",
    "        \n",
    "        # Add semantic results\n",
    "        for idx, score in semantic_results:\n",
    "            all_indices.add(idx)\n",
    "            final_scores[idx] = score * HYBRID_WEIGHT_SEMANTIC\n",
    "        \n",
    "        # Add keyword results\n",
    "        for idx, score in keyword_scores.items():\n",
    "            all_indices.add(idx)\n",
    "            final_scores[idx] += score * HYBRID_WEIGHT_KEYWORD * 0.7\n",
    "        \n",
    "        # Add TF-IDF results\n",
    "        for idx, score in tfidf_scores.items():\n",
    "            all_indices.add(idx)\n",
    "            final_scores[idx] += score * HYBRID_WEIGHT_KEYWORD * 0.3\n",
    "        \n",
    "        # Build results\n",
    "        results = []\n",
    "        sorted_indices = sorted(all_indices, key=lambda x: final_scores[x], reverse=True)\n",
    "        \n",
    "        source_count = defaultdict(int)\n",
    "        for idx in sorted_indices[:TOP_K * 2]:\n",
    "            if final_scores[idx] < MIN_SIMILARITY:\n",
    "                continue\n",
    "            \n",
    "            chunk = self.knowledge_base[idx].copy()\n",
    "            source = chunk.get('title', 'Unknown')\n",
    "            \n",
    "            # Limit per source\n",
    "            if source_count[source] >= 3:\n",
    "                continue\n",
    "            \n",
    "            chunk['similarity'] = final_scores[idx]\n",
    "            results.append(chunk)\n",
    "            source_count[source] += 1\n",
    "            \n",
    "            if len(results) >= TOP_K:\n",
    "                break\n",
    "        \n",
    "        return results\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution with enhanced interface.\"\"\"\n",
    "    rag = FastRAG()\n",
    "    \n",
    "    if not rag.setup():\n",
    "        print(\"❌ Setup failed. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"🤖 ENHANCED FAST RAG SYSTEM READY\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"✨ New features:\")\n",
    "    print(\"  • Lightweight reranking for better relevance\")\n",
    "    print(\"  • 5-strategy fallback system\")  \n",
    "    print(\"  • Enhanced performance monitoring\")\n",
    "    print(\"  • Improved caching and conversation context\")\n",
    "    print(\"\\n💬 Commands:\")\n",
    "    print(\"  • Type questions naturally\")\n",
    "    print(\"  • 'stats' - detailed performance info\")\n",
    "    print(\"  • 'debug' - show system internals\")\n",
    "    print(\"  • 'clear' - clear cache & history\")\n",
    "    print(\"  • 'quit' - exit\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\n💬 Ask: \").strip()\n",
    "            \n",
    "            if not query:\n",
    "                continue\n",
    "            elif query.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"\\n👋 Goodbye!\")\n",
    "                break\n",
    "            elif query.lower() == 'stats':\n",
    "                stats = rag.stats\n",
    "                total = stats['queries']\n",
    "                \n",
    "                if total > 0:\n",
    "                    cache_pct = (stats['cache_hits'] / total) * 100\n",
    "                    fallback_pct = (stats['fallbacks'] / total) * 100\n",
    "                    rerank_pct = (stats['rerank_improvements'] / total) * 100\n",
    "                    \n",
    "                    print(f\"\\n📊 PERFORMANCE STATS\")\n",
    "                    print(f\"Total queries: {total}\")\n",
    "                    print(f\"Average response time: {stats['avg_response_time']:.2f}s\")\n",
    "                    print(f\"Cache hit rate: {cache_pct:.1f}%\")\n",
    "                    print(f\"Fallback usage: {fallback_pct:.1f}%\")\n",
    "                    print(f\"Reranking improvements: {rerank_pct:.1f}%\")\n",
    "                    print(f\"Cache size: {len(rag.cache.cache)}\")\n",
    "                    print(f\"Conversation history: {len(rag.conversation.history)}\")\n",
    "                else:\n",
    "                    print(\"\\n📊 No queries processed yet\")\n",
    "                continue\n",
    "            elif query.lower() == 'debug':\n",
    "                print(f\"\\n🔧 SYSTEM DEBUG INFO\")\n",
    "                print(f\"Knowledge base size: {len(rag.knowledge_base)}\")\n",
    "                print(f\"FAISS index: {'Ready' if rag.index else 'Not ready'}\")\n",
    "                print(f\"Hybrid retriever: {'Ready' if rag.hybrid_retriever else 'Not ready'}\")\n",
    "                print(f\"Fallback system: {'Ready' if rag.fallback_system else 'Not ready'}\")\n",
    "                print(f\"Reranker: {'Ready' if rag.reranker else 'Not ready'}\")\n",
    "                print(f\"Model: {MODEL_NAME}\")\n",
    "                print(f\"Current settings: TOP_K={TOP_K}, MIN_SIM={MIN_SIMILARITY}\")\n",
    "                continue\n",
    "            elif query.lower() == 'clear':\n",
    "                rag.cache = FastCache()\n",
    "                rag.conversation = ConversationManager()\n",
    "                print(\"🧹 Cache and conversation history cleared!\")\n",
    "                continue\n",
    "            \n",
    "            rag.ask(query)\n",
    "            \n",
    "        except (KeyboardInterrupt, EOFError):\n",
    "            print(\"\\n\\n👋 Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error: {e}\")\n",
    "            print(\"⚠️ An error occurred. Please try again.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
